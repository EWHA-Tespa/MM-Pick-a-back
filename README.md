# MM-Pick-a-back

### í´ë” êµ¬ì¡°
```
.
â”œâ”€â”€ data/ 
â”œâ”€â”€ checkpoints_perceiver_io/ # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ìž¥ë˜ëŠ” ìœ„ì¹˜
â”œâ”€â”€ logs_perceiver_io/ # baseline.sh ì‹¤í–‰ í›„ log ê²°ê³¼ íŒŒì¼ì´ ì €ìž¥ë˜ëŠ” ìœ„ì¹˜
â”œâ”€â”€ models/ # CPG í•™ìŠµì— í•„ìš”í•œ ëª¨ë¸ ë²„ì „ì´ ì €ìž¥ë˜ëŠ” ìœ„ì¹˜
â”œâ”€â”€ packnet_models/ # Packnet í•™ìŠµì— í•„ìš”í•œ ëª¨ë¸ ë²„ì „ì´ ì €ìž¥ë˜ëŠ” ìœ„ì¹˜
â”œâ”€â”€ packnet_models_pickaback/ # modeldiff í•˜ê¸° ìœ„í•œ ëª¨ë¸ì´ ì €ìž¥ë˜ëŠ” ìœ„ì¹˜
â”œâ”€â”€ run_pickaback/ # ì‹¤í—˜ ì‹¤í–‰ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ë“¤ì´ ì €ìž¥ë˜ëŠ” ìœ„ì¹˜
â”œâ”€â”€ CPG_cifar100_main_normal.py # ì„ í–‰ ì—°êµ¬ì˜ CPG ìœ„í•œ íŒŒì´ì¬ íŒŒì¼
â”œâ”€â”€ CPG_MM_main_normal.py # MM-Pick-a-backì˜ CPG ìœ„í•œ ë©€í‹°ëª¨ë‹¬ ë²„ì „ íŒŒì´ì¬ íŒŒì¼
â”œâ”€â”€ transfer_kv_perceiver_io.py # k/v projection êµí™˜ ìœ„í•œ íŒŒì´ì¬ íŒŒì¼
â”œâ”€â”€ packnet_cifar100_main_noraml.py # Packnet í•™ìŠµ ìœ„í•œ íŒŒì´ì¬ íŒŒì¼
â”œâ”€â”€ pickaback_perceiverIO.py # perceiver IO ê¸°ë°˜ modeldiff ìœ„í•œ íŒŒì´ì¬ íŒŒì¼
â”œâ”€â”€ tools/
â”œâ”€â”€ utils/
â””â”€â”€ utils_pickaback/
```

---

## ëª¨ë“ˆ ì‹¤í–‰ ë°©ë²•
### ðŸŸ  ë ˆí¬ì§€í† ë¦¬ clone
```
https://github.com/EWHA-Tespa/MM-Pick-a-back.git
cd MM-Pick-a-back
```
### ðŸŸ  ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬
__ðŸ“Š CUB-200-2011:__
```
wget https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz?download=1 -O CUB_200_2011.tgz # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
tar -xvzf CUB_200_2011.tgz # ì••ì¶• í•´ì œ
```
í…ìŠ¤íŠ¸ ë°ì´í„° (ë³„ë„ ë‹¤ìš´ë¡œë“œ): https://drive.google.com/file/d/0B0ywwgffWnLLZW9uVHNjb2JmNlE/view?resourcekey=0-8y2UVmBHAlG26HafWYNoFQ 

ì „ì²˜ë¦¬ ë°©ë²•: https://github.com/EWHA-Tespa/MM-Pickaback-Data-Preprocess/blob/main/cub.ipynb ì˜ raw íŒŒì¼ì„ ë°ì´í„°ì…‹ ì €ìž¥ ìœ„ì¹˜ì— ë¶™ì—¬ë„£ê³ , í•´ë‹¹ íŒŒì¼ì„ ì‹¤í–‰í•˜ì—¬ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤. 

__ðŸ“Š MSCOCO:__
```
wget http://images.cocodataset.org/zips/train2017.zip # í•™ìŠµ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ 
wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip # annotation ë‹¤ìš´ë¡œë“œ
unzip train2017.zip # ì••ì¶• í•´ì œ
unzip annotations_trainval2017.zip # ì••ì¶• í•´ì œ
```

ì „ì²˜ë¦¬ ë°©ë²•: https://github.com/EWHA-Tespa/MM-Pickaback-Data-Preprocess/blob/main/mscoco/preprocess_0430.ipynb ì˜ raw íŒŒì¼ì„ ë°ì´í„°ì…‹ ì €ìž¥ ìœ„ì¹˜ì— ë¶™ì—¬ë„£ê³ , í•´ë‹¹ íŒŒì¼ì„ ì‹¤í–‰í•˜ì—¬ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤. 

__ðŸ“Š Oxford-102-flowers:__

kaggle competitions download -c oxford-102-flower-pytorch

í…ìŠ¤íŠ¸ ë°ì´í„° (ë³„ë„ ë‹¤ìš´ë¡œë“œ): https://drive.google.com/file/d/0B0ywwgffWnLLcms2WWJQRFNSWXM/view?resourcekey=0-Av8zFbeDDvNcF1sSjDR32w

ì „ì²˜ë¦¬ ë°©ë²•: https://github.com/EWHA-Tespa/MM-Pickaback-Data-Preprocess/blob/main/oxford.ipynb ì˜ raw íŒŒì¼ì„ ë°ì´í„°ì…‹ ì €ìž¥ ìœ„ì¹˜ì— ë¶™ì—¬ë„£ê³ , í•´ë‹¹ íŒŒì¼ì„ ì‹¤í–‰í•˜ì—¬ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤. 

### ðŸŸ  ì‹¤í–‰
MM-Pick-a-back ì‹¤í—˜ ê²°ê³¼ëŠ” `run_pickaback/`ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
__1. Baseline__
```
bash run_pickaback/baseline.sh cub # ì‚¬ìš© ë°ì´í„°ì…‹ì— ë”°ë¼ cub, mscoco, oxfordë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤.
```
í„°ë¯¸ë„ì— ìœ„ì˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ë©´, 'logs_perceiver_io/baseline_cub_acc_scratch.txt' íŒŒì¼ì´ ìƒì„±ë˜ê³ , 'checkpoints_perceiver_io/baseline_scratch'ì— ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ê°€ ì €ìž¥ë©ë‹ˆë‹¤. 

__2-1. Backbone task í•™ìŠµ__
ë‹¤ìŒ ëª…ë ¹ì–´ëŠ” ì²« ë²ˆì§¸ ìž‘ì—…ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ê²ƒìž…ë‹ˆë‹¤. ì²« ë²ˆì§¸ ìž‘ì—…ì€ ì´í›„ ìž‘ì—…ì„ ìœ„í•œ ë°±ë³¸(backbone) ëª¨ë¸ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ëª…ë ¹ì–´ëŠ” í•™ìŠµê³¼ ê°€ì§€ì¹˜ê¸°(pruning) ë‹¨ê³„ë¥¼ í•¨ê»˜ ìˆ˜í–‰í•©ë‹ˆë‹¤.
```
bash run_pickaback/wo_backbone.sh cub
```
í„°ë¯¸ë„ì— ìœ„ì˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ë©´, 'checkpoints_perceiver_io/CPG_single_scratch_woexp'ì— ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ê°€ ì €ìž¥ë©ë‹ˆë‹¤. 

__2-2. Pruning ratio__
ë°±ë³¸ ëª¨ë¸ì— ëŒ€í•´ ì ì ˆí•œ ê°€ì§€ì¹˜ê¸° ë¹„ìœ¨(pruning ratio)ì„ ì„ íƒí•©ë‹ˆë‹¤. 
```
bash run_pickaback/select_pruning_ratio_of_backbone.sh
```

__3. ìœ ì‚¬í•œ ëª¨ë¸ íƒìƒ‰__
ModelDiffë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ìƒ ìž‘ì—…ì— ëŒ€í•´ ì˜ì‚¬ê²°ì • íŒ¨í„´ì´ ìœ ì‚¬í•œ ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤.
```
bash run_pickaback/find_backbone.sh
```
ëŒ€ìƒ ìž‘ì—…ì— ëŒ€í•´ ìœ ì‚¬í•œ ëª¨ë¸ì„ ë‹¤ìŒê³¼ ê°™ì´ ì°¾ìŠµë‹ˆë‹¤:
Selected backbone for target 14 = (euc)4

__4. ë°±ë³¸ ìž¬êµ¬ì„±__
targetëª¨ë¸ì˜ key value projectionì„ backbone ëª¨ë¸ì˜ ê²ƒìœ¼ë¡œ êµí™˜í•˜ì—¬ ì´í›„ í¬ë¡œìŠ¤-ëª¨ë‹¬ ê°„ í•™ìŠµì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
```
bash run_pickaback/transfer_kv.sh
```

__5. target ëª¨ë¸ í•™ìŠµ__
ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ ëŒ€ìƒ ìž‘ì—…ì— ëŒ€í•œ ì§€ì† í•™ìŠµ(continual learning) ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
```
bash run_pickaback/w_backbone_MM.sh
```
í„°ë¯¸ë„ì— ìœ„ì˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ë©´, 'checkpoints_perceiver_io/CPG_fromsingle_scratch_woexp_target'ì— ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ê°€ ì €ìž¥ë©ë‹ˆë‹¤. 

### ëª¨ë¸ ë° ì½”ë“œ ê¸°ì—¬
ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒ ì˜¤í”ˆì†ŒìŠ¤ì— ê¸°ë°˜í•©ë‹ˆë‹¤:
* [Pick-a-back: Selective Device-to-Device Knowledge Transfer in Federated Continual Learning](https://github.com/jinyi-yoon/Pick-a-back)
* [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://github.com/lucidrains/perceiver-pytorch)

---



### Ready for data
Download the CIFAR-100 dataset in the directory 'data/' and unzip the file. The structure of the dataset is as follows:
```
â””â”€cifar100_org
    â”œâ”€test
    â”‚  â”œâ”€aquatic_mammals
    â”‚  â”‚  â”œâ”€beaver
    â”‚  â”‚  â”œâ”€dolphin
    â”‚  â”‚  â”œâ”€otter
    â”‚  â”‚  â”œâ”€seal
    â”‚  â”‚  â””â”€whale
    â”‚  â”œâ”€ ...
    â”‚  â””â”€vehicles_2
    â”‚      â”œâ”€lawn_mower
    â”‚      â”œâ”€rocket
    â”‚      â”œâ”€streetcar
    â”‚      â”œâ”€tank
    â”‚      â””â”€tractor
    â””â”€train
        â”œâ”€aquatic_mammals
        â”‚  â”œâ”€beaver
        â”‚  â”œâ”€dolphin
        â”‚  â”œâ”€otter
        â”‚  â”œâ”€seal
        â”‚  â””â”€whale
        â”œâ”€ ...
        â””â”€vehicles_2
            â”œâ”€lawn_mower
            â”œâ”€rocket
            â”œâ”€streetcar
            â”œâ”€tank
            â””â”€tractor
```
All the subclasses should be located under the superclass. Please check the hierarchy of the dataset [here](https://www.cs.toronto.edu/~kriz/cifar.html).

## STEP 1) Baseline accuracy
Use the following command to train the scratch model for each task. This accuracy is used for the threshold of model expansion decision and pruning ratio selection.
```console
bash run_pickaback/baseline.sh
```
It generates 'logs_lenet5/baseline_cifar100_acc_scratch.txt' file. The checkpoints are available at 'checkpoints_lenet/baseline_scratch.'

## STEP 2-1) Train the backbone task
The following command is to train the first task. The first task is used as a backbone model for the next task. It processes the training and pruning steps.
```console
bash run_pickaback/wo_backbone.sh
```
The checkpoints are generated at 'checkpoints_lenet5/CPG_single_scratch_woexp.'


## STEP 2-2) Select the pruning ratio
We select the proper pruning ratio for the backbone model. Please refer to the details from [gradual pruning](https://arxiv.org/abs/1710.01878).
```console
bash run_pickaback/select_pruning_ratio_of_backbone.sh
```
The selected pruning ratio is provided as:
> 2023-05-24 17:13:43,124 - root - INFO - We choose pruning ratio 0.8


## STEP 3-1) Find the similar model
Based on [ModelDiff](https://dl.acm.org/doi/abs/10.1145/3460319.3464816), we select the model with similar decision pattern for the target task.
```console
bash run_pickaback/find_backbone.sh
```
It find the similar model for the target task as:
> Selected backbone for target 14 = (euc) 4


## STEP 3-2) Train the target task
Run the following command to train the continual model for the target task.
```console
bash run_pickaback/w_backbone.sh
```
The checkpoints are available at 'checkpoints_lenet5/CPG_fromsingle_scratch_woexp_target.' You can get the final accuracy as follows:
> 2023-04-11 21:40:14,129 - root - INFO - In validate()-> Val Ep. #100 loss: 0.742, accuracy: 75.60, sparsity: 0.000, task2 ratio: 1.794, zero ratio: 0.000, mpl: 1.4142135623730951, shared_ratio: 0.778
